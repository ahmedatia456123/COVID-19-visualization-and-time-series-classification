{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfeKFPglzR9-"
      },
      "outputs": [],
      "source": [
        "!pip install prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2mR9P6d7ZK7"
      },
      "outputs": [],
      "source": [
        "!pip install tslearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiK_YMs1IR9"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcnJ-Fn1K0F"
      },
      "source": [
        "The COVID-19 pandemic has profoundly impacted the world, presenting unprecedented challenges in healthcare, economy, and social dynamics. Understanding the spread and impact of the virus through data-driven approaches has become crucial for effective decision-making and policy formulation. This project aims to visualize and explore COVID-19 time series data, employing clustering techniques to identify patterns and insights across different regions. By analyzing data from Johns Hopkins University, a leading source for COVID-19 tracking, this project seeks to uncover trends and correlations that can aid in understanding the pandemic's trajectory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TB-Wib-1QjB"
      },
      "source": [
        "# Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsVFFFRA3Zoj"
      },
      "source": [
        "Since the outbreak of COVID-19 in late 2019, data scientists and researchers have focused on tracking and analyzing the virus's spread globally. Johns Hopkins University has been at the forefront, providing comprehensive datasets that detail daily reports of COVID-19 cases, recoveries, and deaths across countries and regions. These datasets have become invaluable resources for researchers and policymakers, offering insights into the dynamics of the pandemic.\n",
        "\n",
        "The project leverages this data to perform time series analysis, utilizing visualization techniques to present the data intuitively and engagingly. Furthermore, clustering algorithms are applied to group regions with similar pandemic trends, enabling a deeper understanding of how different factors may influence the virus's spread. By exploring patterns in the data, this project aims to contribute to the broader efforts in managing and mitigating the impacts of COVID-19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcDceCm63sce"
      },
      "source": [
        "**Key Stakeholders:**\n",
        "\n",
        "**Healthcare Providers:** Need insights to manage hospital resources and patient care.\n",
        "\n",
        "**Government Agencies:** Use data to inform public health policies and restrictions.\n",
        "\n",
        "**Researchers and Academics:** Require data for ongoing studies and publications.\n",
        "\n",
        "**General Public:** Benefits from understanding the spread and impact of COVID-19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt296bwi4K0i"
      },
      "source": [
        "#Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cUi5h9N1JF1"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from plotly.offline import init_notebook_mode,iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import datetime\n",
        "import operator\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tslearn.clustering import silhouette_score\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "from prophet import Prophet\n",
        "from prophet.plot import plot_plotly, plot_components_plotly ,add_changepoints_to_plot\n",
        "\n",
        "#decomposed\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "plt.style.use('seaborn-poster')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOxBSD8y4wvn"
      },
      "source": [
        "# Import data From Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCbYqiFt4TaC"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "confirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
        "deaths_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
        "latest_data = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/01-15-2023.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPehrI967Ic2"
      },
      "source": [
        "#Exploring and Preparing Data Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaFEMcx_740d"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#Exploring Confirmed Data\n",
        "confirmed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwQECCWdORS-"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "deaths_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFuGtzfyOTtF"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "latest_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WF87OV1PYfy"
      },
      "source": [
        "Get all Time Series data from COVID-19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gpx-VjfO0kf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Grouping all death and confirmed numbers by country for each day\n",
        "\n",
        "confirmed_ts_by_country = confirmed_df.drop(['Long','Lat','Province/State'],axis=1).groupby('Country/Region').sum()\n",
        "death_ts_by_country = deaths_df.drop(['Long','Lat','Province/State'],axis=1).groupby('Country/Region').sum()\n",
        "global_ts_confirmed = confirmed_ts_by_country.sum()\n",
        "global_ts_deaths = death_ts_by_country.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_sPadIOVg6"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Helper functions\n",
        "def get_time_series(ts):\n",
        "    \"\"\"\n",
        "    Convert the index of a Pandas Series to DateTime format and sort the index.\n",
        "    \"\"\"\n",
        "    ts.index = pd.to_datetime(ts.index, format='%m/%d/%y')\n",
        "    ts.sort_index(inplace=True)\n",
        "    return ts\n",
        "\n",
        "def get_rate_of_change(ts):\n",
        "    \"\"\"\n",
        "    Calculate the rate of change of a Pandas Series.\n",
        "    \"\"\"\n",
        "    temp_df = pd.DataFrame(get_time_series(ts))\n",
        "    temp_df.columns = ['numbers']\n",
        "    temp_df['Shifted'] = temp_df['numbers'].shift(1)\n",
        "    temp_df['Difference'] = temp_df['numbers'] - temp_df['Shifted']\n",
        "\n",
        "    return temp_df['Difference']\n",
        "\n",
        "\n",
        "def plotly_plot(ts_list, chart, title, x_title='Date', y_title='Person', ma=False, dir_plot=False, identifier=None, data=True):\n",
        "    \"\"\"\n",
        "    Plot the time series data and optionally its moving average using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "    - ts_list: List of Pandas Series with DateTime index\n",
        "    - chart: Type of chart to plot ('line', 'bar', etc.)\n",
        "    - title: Title of the chart\n",
        "    - x_title: Title of the x-axis\n",
        "    - y_title: Title of the y-axis\n",
        "    - ma: If True, plots a 7-day moving average\n",
        "    - dir_plot: If True, skips rate of change calculation\n",
        "    - identifier: List of identifiers for each time series (optional)\n",
        "    - data: If True, includes data in the plot\n",
        "    \"\"\"\n",
        "    if not isinstance(ts_list, list):\n",
        "        ts_list = [ts_list]\n",
        "\n",
        "    if identifier is None:\n",
        "        identifier = ['Data'] * len(ts_list)\n",
        "\n",
        "    # Create the plot\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for i, ts in enumerate(ts_list):\n",
        "        # Skip rate of change\n",
        "        if not dir_plot:\n",
        "            ts = get_rate_of_change(ts)\n",
        "\n",
        "        # Prepare data for plotting\n",
        "        df = pd.DataFrame(ts)\n",
        "        df.reset_index(inplace=True)\n",
        "        df.columns = ['Date', 'Value']\n",
        "\n",
        "        if ma:\n",
        "            df['Moving Average'] = df['Value'].rolling(window=14).mean()\n",
        "\n",
        "        # Plot main data\n",
        "        if chart == 'line' and data:\n",
        "            fig.add_trace(go.Scatter(x=df['Date'], y=df['Value'], mode='lines', name=f'{identifier[i]}'))\n",
        "        elif chart == 'bar' and data:\n",
        "            fig.add_trace(go.Bar(x=df['Date'], y=df['Value'], name=f'{identifier[i]}', opacity=1))\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported chart type. Use 'line' or 'bar'.\")\n",
        "\n",
        "        # Plot moving average if specified\n",
        "        if ma:\n",
        "            fig.add_trace(go.Scatter(x=df['Date'], y=df['Moving Average'], mode='lines', name=f'{identifier[i]} MA14'))\n",
        "\n",
        "    # Customize layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=x_title,\n",
        "        yaxis_title=y_title,\n",
        "        xaxis=dict(\n",
        "            tickformat='%Y-%m-%d',\n",
        "            tickangle=90\n",
        "        ),\n",
        "        legend=dict(\n",
        "            x=0.05,\n",
        "            y=0.95,\n",
        "            bgcolor='rgba(255, 255, 255, 0)',\n",
        "            bordercolor='rgba(255, 255, 255, 0)'\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, t=50, b=100),\n",
        "        height=600,\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    # Show plot\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYbBtNWXoecU"
      },
      "source": [
        "## Worldwide Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6LL9DXp_LV1"
      },
      "source": [
        "Investigate what happend 2023-1-15 to the number of death"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tp-xi8q-nkhO"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "wanted_days=death_ts_by_country[['1/14/23','1/15/23']]\n",
        "wanted_days['diff']=wanted_days['1/15/23']-wanted_days['1/14/23']\n",
        "wanted_days.sort_values('diff',ascending=False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz1FCjesDJ8l"
      },
      "source": [
        "So i China there is Huge number of death between 1/14/23 and 1/15/23\n",
        "Need more investigation to know which State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I_Bi7pY0D9jh"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "china_day15_1_23 = deaths_df[deaths_df['Country/Region']=='China'][['Province/State','1/14/23','1/15/23']]\n",
        "china_day15_1_23['diff']=china_day15_1_23['1/15/23']-china_day15_1_23['1/14/23']\n",
        "china_day15_1_23.sort_values('diff',ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "An0YlR2YEUnL"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "deaths_df.iloc[89,4:].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1usA97OFx6q"
      },
      "source": [
        "I think  like there is a problem with the data need further investigation for now it has no explanation, maybe it is delayed report but the state is unknown so iam gonna delete this time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LtX3QC_KPigL"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "deaths_df.drop(deaths_df.index[89], inplace=True)\n",
        "death_ts_by_country = deaths_df.drop(['Long','Lat','Province/State'],axis=1).groupby('Country/Region').sum()\n",
        "global_ts_deaths = death_ts_by_country.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq3N70t6Pye1"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print('Change Over Time')\n",
        "plotly_plot(global_ts_confirmed,'line',title='Cumulative Confirmed Cases Worldwide',ma=True,dir_plot=True)\n",
        "plotly_plot(global_ts_deaths,'line',title='Cumulative Death Cases Worldwide',ma=True,dir_plot=True)\n",
        "print('Rate of Change')\n",
        "plotly_plot(global_ts_confirmed,'bar',title='Rate of Confirmed Cases Worldwide',ma=True)\n",
        "plotly_plot(global_ts_deaths,'bar',title='Rate of Death Cases Worldwide',ma=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2vV_lIMFeU"
      },
      "source": [
        "## Cheking Seasonality and Trend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0r_R7EAMKJj"
      },
      "source": [
        "### Confirmed Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K9wS0DgMEs0"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "sample_confirmed = get_rate_of_change(global_ts_confirmed).to_frame()\n",
        "sample_confirmed['Date'] = sample_confirmed.index\n",
        "sample_confirmed.columns=['y','ds']\n",
        "\n",
        "c_m = Prophet(seasonality_mode='additive',changepoint_prior_scale=0.6)\n",
        "c_m.fit(sample_confirmed)\n",
        "future = c_m.make_future_dataframe(periods=0)\n",
        "forecast = c_m.predict(future)\n",
        "plot_components_plotly(c_m,forecast,figsize=(900, 400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZMN6jCQPCye"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "sample_confirmed = get_rate_of_change(global_ts_deaths).to_frame()\n",
        "sample_confirmed['Date'] = sample_confirmed.index\n",
        "sample_confirmed.columns=['y','ds']\n",
        "\n",
        "c_m = Prophet(seasonality_mode='additive',changepoint_prior_scale=0.5)\n",
        "c_m.fit(sample_confirmed)\n",
        "future = c_m.make_future_dataframe(periods=0)\n",
        "forecast = c_m.predict(future)\n",
        "plot_components_plotly(c_m,forecast,figsize=(900, 400))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDDb16y-H1Ah"
      },
      "source": [
        "# Analysis of COVID-19 Confirmed Case Trends\n",
        "\n",
        "## 1. Initial Phase of the Pandemic\n",
        "In the early stages of the pandemic, the number of confirmed cases was relatively low. This may be attributed to several factors:\n",
        "- **Detection Challenges**: During this period, the methods for identifying and diagnosing COVID-19 were still being developed.\n",
        "- **Data Reporting Issues**: Some countries may have been underreporting or concealing actual case numbers.\n",
        "- **Virus Spread**: The virus had not yet had sufficient time to spread widely.\n",
        "\n",
        "## 2. Seasonality Observed from 2021\n",
        "Starting from early 2021, there is a noticeable seasonal pattern in the rate of confirmed cases. We observe an increase in the number of confirmed cases approximately every 3-4 months. This recurring trend suggests a cyclical pattern in the spread of the virus.\n",
        "\n",
        "## 3. Significant Surge in Confirmed Cases\n",
        "There was a marked increase in the number of confirmed cases during December 2021 and April 2022. This spike warrants further investigation to understand the underlying causes, which could include factors such as new variants, changes in public health policies, or seasonal effects.\n",
        "\n",
        "## 4. Decrease in Seasonality Post-April 2022\n",
        "After April 2022, the seasonal pattern in case rates seems to diminish. This reduction in seasonality could indicate a stabilization in the spread of the virus or a shift in the pandemic dynamics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s08_DIXLf1x"
      },
      "source": [
        "# COVID-19 Death Trends Analysis\n",
        "\n",
        "## 1. Early Death Rates\n",
        "- **Initial Surge**: Death rates initially spiked due to limited knowledge about the virus and its symptoms. Early on, the virus was often mistaken for a common flu, leading to delayed and inadequate responses.\n",
        "\n",
        "## 2. Seasonality of Death Rates\n",
        "- **Recurring Patterns**: Similar to confirmed case numbers, death rates also showed seasonal fluctuations approximately every 4 months. This seasonality may be linked to changes in weather patterns in certain regions and inadequate preparedness for these changes.\n",
        "\n",
        "## 3. Trends from 2020 to 2022\n",
        "- **Rising and Falling Rates**: From January 2020 to January 2021, there was a notable upward trend in death rates. This was followed by a gradual decline from January 2021 to February 2022. The decrease in death rates can be attributed to several factors:\n",
        "  - **Global Awareness**: Increased global awareness about the pandemic led to better prevention strategies.\n",
        "  - **Vaccination Impact**: Although vaccines were available since late 2020, the decline in death rates was initially slow due to various reasons:\n",
        "    - **Slow Vaccine Rollout**:\n",
        "      - **Limited Supply**: Vaccine production and distribution were initially slow.\n",
        "      - **Logistical Challenges**: Setting up vaccination sites and scheduling appointments took time.\n",
        "    - **Vaccine Hesitancy**:\n",
        "      - **Public Concerns**: Concerns about vaccine safety and misinformation caused reluctance.\n",
        "      - **Access Issues**: Vaccine access was limited in some areas.\n",
        "    - **New Variants**:\n",
        "      - **Increased Spread**: Variants like Delta spread rapidly, complicating control efforts.\n",
        "      - **Reduced Effectiveness**: Some variants reduced vaccine effectiveness, necessitating booster shots.\n",
        "    - **Delayed Benefits**:\n",
        "      - **Herd Immunity**: Achieving sufficient vaccination coverage for significant impact took time.\n",
        "      - **Data Lag**: Analyzing the impact of vaccines took time.\n",
        "    - **Healthcare System Stress**:\n",
        "      - **Overwhelmed Hospitals**: The healthcare system was strained, affecting mortality rates.\n",
        "\n",
        "## 4. Decline in Death Rates Post-2022-2\n",
        "- **Increased Vaccination Coverage**:\n",
        "  - **Higher Rates**: By 2022, a larger portion of the global population was vaccinated, including booster doses, which increased immunity and reduced severe cases.\n",
        "  - **Effective Vaccines**: Vaccines proved highly effective in preventing severe illness and deaths.\n",
        "- **Widespread Immunity**:\n",
        "  - **Herd Immunity**: Higher vaccination rates and natural immunity from previous infections contributed to reduced virus spread.\n",
        "- **Improved Treatments**:\n",
        "  - **Advanced Therapies**: Enhanced medical treatments improved the management of severe cases and reduced mortality.\n",
        "- **Adaptation to Variants**:\n",
        "  - **Updated Vaccines**: New vaccines and boosters targeted emerging variants, improving protection.\n",
        "  - **Adapted Strategies**: Public health strategies were updated based on new data.\n",
        "- **Public Health Measures**:\n",
        "  - **Ongoing Precautions**: Continued use of masks, social distancing, and hygiene measures helped reduce transmission.\n",
        "- **Behavioral Changes**:\n",
        "  - **Increased Awareness**: Greater public awareness led to better adherence to preventive guidelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12arOKviYvLN"
      },
      "source": [
        "# By Country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuUiyKW7Z6E4"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "sample = confirmed_ts_by_country.iloc[:,-1:].rename(columns={'3/9/23': 'total_cases'})\n",
        "fig = px.treemap(sample, path=[sample.index], values='total_cases')\n",
        "print('Tree Map For Number of confirmed cases By Country')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "805vkK5wafk5"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "sample = death_ts_by_country.iloc[:,-1:].rename(columns={'3/9/23': 'total_deaths'})\n",
        "\n",
        "fig = px.treemap(sample, path=[sample.index], values='total_deaths')\n",
        "print('Tree Map For Number of Death By Country')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJjdraTWh839"
      },
      "source": [
        "Spatial Analysis for confirmed cases animated over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywGubBRsh8Wa"
      },
      "outputs": [],
      "source": [
        "spatial_df = pd.DataFrame(columns=['date','confirmed','country'])\n",
        "for i in range(confirmed_ts_by_country.shape[0]):\n",
        "  temp_df=None\n",
        "  country = confirmed_ts_by_country.iloc[i].name\n",
        "  temp_df = get_time_series(confirmed_ts_by_country.iloc[i]).to_frame()\n",
        "  temp_df = temp_df.reset_index().rename(columns={country:'confirmed','index':'date'})\n",
        "  temp_df['country'] = country\n",
        "  temp_df = temp_df\n",
        "  spatial_df = pd.concat([spatial_df, temp_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMizy99xvhwk"
      },
      "outputs": [],
      "source": [
        "spatial_df.confirmed = spatial_df.confirmed.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido\n"
      ],
      "metadata": {
        "id": "wkdJENZuOk5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyMMv4gk48-"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Define a color scale with expanded range\n",
        "color_scale = [\n",
        "    [0, 'rgba(255,255,255,1)'],    # White for very low values\n",
        "    [0.01, 'rgba(255,255,204,1)'],  # Light yellow for small values\n",
        "    [0.05, 'rgba(255,204,153,1)'],  # Light orange for low values\n",
        "    [0.2, 'rgba(255,153,51,1)'],    # Orange for moderate values\n",
        "    [0.5, 'rgba(255,102,0,1)'],     # Dark orange for high values\n",
        "    [0.8, 'rgba(255,0,0,1)'],       # Dark red for very high values\n",
        "    [1, 'rgba(139,0,0,1)']          # Darker red for the highest values\n",
        "]\n",
        "\n",
        "fig = px.choropleth(\n",
        "    spatial_df,\n",
        "    locations='country',\n",
        "    locationmode='country names',\n",
        "    color='confirmed',\n",
        "    hover_name='country',\n",
        "    animation_frame='date',\n",
        "    title='COVID-19 Confirmed Cases Over Time',\n",
        "    width=1400,\n",
        "    height=800,\n",
        "    color_continuous_scale=color_scale\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    updatemenus=[\n",
        "        {\n",
        "            \"buttons\": [\n",
        "                {\n",
        "                    \"args\": [None, {\"frame\": {\"duration\": 0, \"redraw\": True}, \"fromcurrent\": True}],\n",
        "                    \"label\": \"Play\",\n",
        "                    \"method\": \"animate\",\n",
        "                },\n",
        "                {\n",
        "                    \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": True}, \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
        "                    \"label\": \"Pause\",\n",
        "                    \"method\": \"animate\",\n",
        "                },\n",
        "            ],\n",
        "            \"direction\": \"left\",\n",
        "            \"pad\": {\"r\": 10, \"t\": 87},\n",
        "            \"showactive\": False,\n",
        "            \"type\": \"buttons\",\n",
        "            \"x\": 0.1,\n",
        "            \"xanchor\": \"right\",\n",
        "            \"y\": 0,\n",
        "            \"yanchor\": \"top\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5lwJ8BhVtRj"
      },
      "source": [
        "\n",
        "\n",
        "The pandemic's impact varied across countries due to several key factors, with more shared factors leading to greater effects. Some of these factors include:\n",
        "\n",
        "- **Population Density**: Densely populated areas experienced faster virus spread.\n",
        "- **Travel Connectivity**: High levels of international travel led to early outbreaks.\n",
        "- **Healthcare Capacity**: Limited infrastructure resulted in higher mortality rates.\n",
        "- **Government Response**: Timely measures controlled the spread, while delays worsened it.\n",
        "- **Public Compliance**: Adherence to guidelines and vaccine acceptance influenced outcomes.\n",
        "- **Socio-Economic Factors**: Economic disparities affected the ability to follow restrictions.\n",
        "- **Vaccine Rollout**: The speed and efficiency of distribution impacted control efforts.\n",
        "- **Emerging Variants**: New, more transmissible variants complicated containment.\n",
        "- **Public Health Infrastructure**: Testing and tracing capabilities were crucial.\n",
        "- **Cultural Attitudes**: Views on authority and health influenced compliance.\n",
        "\n",
        "### Some Reasons for top 5 countries\n",
        "\n",
        "- **United States**:\n",
        "  - **High Population Density**: Urban areas like New York City saw rapid transmission due to dense populations.\n",
        "  - **International Travel**: As a major global hub, exposure to international travelers was significant.\n",
        "  - **Health Inequalities**: Disparities in healthcare access and pre-existing conditions contributed to higher mortality rates.\n",
        "  - **Tourism and Trade Movement**: The U.S. had high levels of both international tourism and trade, increasing the potential for virus spread.\n",
        "\n",
        "- **India**:\n",
        "  - **Population Size**: Being highly populous, controlling the virus's spread across diverse regions was challenging.\n",
        "  - **Healthcare System Strain**: The pandemic overwhelmed infrastructure, especially during the 2021 second wave.\n",
        "  - **Economic Factors**: Lockdowns severely impacted the economy, complicating response efforts.\n",
        "  - **Tourism and Trade Movement**: India experienced substantial international travel and trade, which facilitated the virus's spread.\n",
        "\n",
        "- **France**:\n",
        "  - **Population Density**: High population density in urban areas like Paris facilitated the virus's spread.\n",
        "  - **Healthcare System**: France's healthcare system faced significant pressure, especially in major cities.\n",
        "  - **Government Response**: Early and strict lockdown measures were implemented, which initially helped control the spread but faced challenges with subsequent waves.\n",
        "  - **Tourism and Trade Movement**: France is a major tourist destination and trade hub, with extensive international travel contributing to the spread.\n",
        "\n",
        "- **Germany**:\n",
        "  - **Effective Early Response**: Germany implemented early and effective containment measures, including widespread testing and contact tracing.\n",
        "  - **Healthcare Capacity**: The country maintained a relatively robust healthcare system but faced challenges with rising cases in later waves.\n",
        "  - **Economic Impact**: The pandemic's economic impact was significant, influencing public compliance and response measures.\n",
        "  - **Tourism and Trade Movement**: Germany's significant role in global trade and tourism increased the virus's potential for widespread impact.\n",
        "\n",
        "- **Brazil**:\n",
        "  - **Government Response**: Delayed and inconsistent measures led to rapid virus spread.\n",
        "  - **Urbanization**: Cities like São Paulo and Rio de Janeiro experienced high transmission due to crowded conditions.\n",
        "  - **Variants**: A hotspot for new variants, increasing transmission and severity.\n",
        "  - **Tourism and Trade Movement**: Brazil's trade and tourism activities contributed to the virus's rapid spread.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8qDz7tzrEPs"
      },
      "source": [
        "## Now I will explor data for some countries i will chose top 5 in number of death and confirmed cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKNP6stQrRqs"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "top_5_by_death = list(sample.sort_values('total_deaths',ascending=False).head().index)\n",
        "\n",
        "ts_list = []\n",
        "ts_list2 = []\n",
        "id_list = []\n",
        "for country in top_5_by_death:\n",
        "  id_list.append(f'{country} Bars')\n",
        "  ts_list.append(death_ts_by_country.loc[country])\n",
        "  ts_list2.append(confirmed_ts_by_country.loc[country])\n",
        "\n",
        "print('Death')\n",
        "plotly_plot(ts_list,'line',title='Death increase over time',ma=True,identifier=id_list,dir_plot=True)\n",
        "plotly_plot(ts_list,'bar',title='Rate of Death',ma=True,identifier=id_list)\n",
        "\n",
        "print('Confimed cases')\n",
        "plotly_plot(ts_list2,'line',title='cases increase over time',ma=True,identifier=id_list,dir_plot=True)\n",
        "plotly_plot(ts_list2,'bar',title='Rate of cases',ma=True,identifier=id_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJTx8OUje1fT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcvWVhxOYqQP"
      },
      "source": [
        "### 1. Variation in Seasonality by Country\n",
        "\n",
        "Each country shows different seasonality in confirmed COVID-19 cases due to:\n",
        "\n",
        "1. **Climate and Weather**: Local conditions influence virus spread.\n",
        "2. **Government Policies**: Varying effectiveness and timing of measures like lockdowns.\n",
        "3. **Healthcare Capacity**: Differences in managing peak cases.\n",
        "4. **Variants**: Impact of new, more transmissible strains varies.\n",
        "5. **Vaccination**: Differences in rollout speed and public acceptance.\n",
        "\n",
        "### 2. Case and Death Rate Trends (December 2021 - March 2022)\n",
        "\n",
        "During this period, many countries saw a rise in confirmed cases but lower death rates due to:\n",
        "\n",
        "1. **Omicron Variant**: More transmissible but generally less severe.\n",
        "2. **Widespread Vaccination**: Reduced severity and prevented many severe cases.\n",
        "3. **Natural Immunity**: Previous exposure led to some level of immunity, reducing severe outcomes.\n",
        "4. **Improved Treatments**: Enhanced medical protocols and treatments.\n",
        "5. **Public Health Measures**: Continued use of masks and social distancing helped mitigate impacts.\n",
        "\n",
        "### 3. Seasonal Patterns: Global vs. Country-Level\n",
        "\n",
        "Globally, confirmed cases show seasonality every 3-4 months, while country-specific data shows patterns every 10-12 months due to:\n",
        "\n",
        "1. **Global vs. Local Variability**: Global averages smooth out local trends, showing more frequent seasonality.\n",
        "2. **Diverse Climatic and Social Conditions**: Local factors create longer-term seasonal effects.\n",
        "3. **Data Averaging**: Global data aggregates information, reflecting more frequent patterns.\n",
        "4. **Public Health Measures**: Variations in measures impact local seasonal cycles.\n",
        "5. **Vaccination and Immunity**: Global vaccination rates and immunity affect patterns differently at the country level.\n",
        "\n",
        "### 4. Similar Patterns Post-January 2022\n",
        "\n",
        "After January 2022, similar patterns emerged among top countries due to:\n",
        "\n",
        "1. **Adaptation**: Adjustments to lockdowns and home-based activities influenced virus spread.\n",
        "2. **Public Health Measures**: Similar global responses impacted transmission patterns.\n",
        "3. **Behavioral Changes**: Common behaviors due to restrictions led to similar infection patterns.\n",
        "4. **Vaccination and Immunity**: Increased global vaccination and immunity contributed to parallel trends across diverse locations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLuFCxkdmydg"
      },
      "source": [
        "#Investigate China Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-MbXEMBm65Z"
      },
      "outputs": [],
      "source": [
        "China_con = confirmed_ts_by_country.loc['China']\n",
        "China_death = death_ts_by_country.loc['China']\n",
        "\n",
        "print('Death')\n",
        "plotly_plot(China_death,'line',title='Death increase over time',ma=True,dir_plot=True)\n",
        "plotly_plot(China_death,'bar',title='Rate of Death',ma=True,identifier=id_list)\n",
        "\n",
        "print('Case')\n",
        "plotly_plot(China_con,'line',title='cases increase over time',ma=True,dir_plot=True)\n",
        "plotly_plot(China_con,'bar',title='Rate of cases',ma=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRbtg5yOoo6N"
      },
      "source": [
        "# Analysis of China's COVID-19 Data Reporting\n",
        "\n",
        "The data from China shows unusual patterns, with constant numbers of deaths and cases over two years, which seems improbable. After investigating, it appears that China's initial reporting policies contributed to this anomaly. Here’s a summary of the key factors:\n",
        "\n",
        "## 1. Initial Reporting Delays\n",
        "- **Early Stages**: In the early stages of the pandemic, there were delays in reporting and limited public information. This resulted in underreporting of both cases and deaths.\n",
        "\n",
        "## 2. Information Control\n",
        "- **Censorship**: The Chinese government imposed censorship and restrictions on information about the virus, including suppression of early warnings and criticism of the government's response.\n",
        "- **Media Restrictions**: Journalists and independent observers faced limitations, affecting the accuracy and flow of information.\n",
        "\n",
        "## 3. Changes in Reporting Policies\n",
        "- **Increased Transparency**: As the pandemic progressed, China revised its reporting policies and increased transparency.\n",
        "- **Data Revisions**: There were significant adjustments to reported figures as new information became available.\n",
        "\n",
        "## 4. International Criticism\n",
        "- **Global Scrutiny**: The international community criticized China for its initial handling of the outbreak and the impact on global transparency, focusing on the accuracy and timeliness of the reported data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NoZzAnErTBL"
      },
      "source": [
        "#Checking Case_Fatality_Ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqWcOhaG6yji"
      },
      "source": [
        "calcualte Case_Fatality_Ratio per state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4TihkyE67ld"
      },
      "outputs": [],
      "source": [
        "State_Case_Fatality_Ratio = latest_data[latest_data['Country_Region']=='US'][['Province_State','Confirmed','Deaths']].groupby('Province_State').sum()\n",
        "State_Case_Fatality_Ratio['Case_Fatality_Ratio'] = (State_Case_Fatality_Ratio['Deaths']/State_Case_Fatality_Ratio['Confirmed'])*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgtL8jhpuMrb"
      },
      "source": [
        "#let invistigate US data more in depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG2AXsSwvdNo"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Select and sort data\n",
        "sorted_df = State_Case_Fatality_Ratio.sort_values('Case_Fatality_Ratio', ascending=False)[1:11]\n",
        "\n",
        "# Apply heatmap styling\n",
        "styled_df = sorted_df.style.background_gradient(subset=['Case_Fatality_Ratio'], cmap='Reds')\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vYcB2vUxabN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# Create the pie chart\n",
        "fig = px.pie(\n",
        "    sorted_df,\n",
        "    names=sorted_df.index,\n",
        "    values='Case_Fatality_Ratio',\n",
        "    title='Case Fatality Ratio by Country',\n",
        "    color='Case_Fatality_Ratio',\n",
        "    color_discrete_sequence=px.colors.qualitative.Plotly\n",
        ")\n",
        "\n",
        "# Update layout to increase the size of the pie chart\n",
        "fig.update_layout(\n",
        "    width=800,  # Adjust the width as needed\n",
        "    height=800  # Adjust the height as needed\n",
        ")\n",
        "\n",
        "# Show the pie chart\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz5HpegY9g9Z"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "024YWoAyAThL"
      },
      "source": [
        "I will do Cluster classification based on shape of time series for Cofirmed cases change over days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dveMuQKKHOnv"
      },
      "source": [
        "Now i will scale the data because there is huge diffrance in scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ9wxt1SU9MV"
      },
      "outputs": [],
      "source": [
        "confirmed_ts_by_country.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmUgxQm5TjZs"
      },
      "source": [
        "We have 201 time series, each with a length of T=1143. Determining the optimal number of clusters can be computationally intensive, as it requires multiple iterations over the entire dataset. To streamline this process, I will use PCA to reduce the dimensionality of the time series. This will help identify the optimal number of clusters efficiently. Once the optimal number of clusters is determined, I will perform cluster analysis on the original time series data to ensure accurate and meaningful results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkznOiMiWP6n"
      },
      "source": [
        "Create Dataframe for death differance for all countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySVpZtk5Y6Yg"
      },
      "source": [
        "# I remved noise in two steps first using Savitzky-Golay filter and 2nd using moving average 50 after differancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evu-C48IXrF7"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import savgol_filter\n",
        "\n",
        "def smooth_time_series(series, window_length=50, polyorder=2):\n",
        "    \"\"\"\n",
        "    Smooth a time series using the Savitzky-Golay filter.\n",
        "\n",
        "    Parameters:\n",
        "        series (pd.Series): Time series data with a datetime index.\n",
        "        window_length (int): The length of the filter window (must be odd and greater than polyorder).\n",
        "        polyorder (int): The order of the polynomial used to fit the samples.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Smoothed time series.\n",
        "    \"\"\"\n",
        "    # Apply Savitzky-Golay filter\n",
        "    smoothed_data = savgol_filter(series, window_length=window_length, polyorder=polyorder)\n",
        "\n",
        "    # Return the smoothed series as a Pandas Series\n",
        "    return pd.Series(smoothed_data, index=series.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KfI7P7XblCz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_rate_of_change_with_ma(ts):\n",
        "    \"\"\"\n",
        "    Calculate the rate of change of a Pandas Series.\n",
        "    \"\"\"\n",
        "    name =ts.name\n",
        "    temp_df = pd.DataFrame((get_time_series(ts)))\n",
        "    temp_df.columns = ['numbers']\n",
        "    temp_df['Shifted'] = temp_df['numbers'].shift(1)\n",
        "    temp_df['Difference'] = temp_df['numbers'] - temp_df['Shifted']\n",
        "    temp_df['Moving Average'] = temp_df['Difference'].dropna().rolling(window=50).mean().fillna(0).dropna()\n",
        "    temp_df.rename(columns={'Moving Average': name}, inplace=True)\n",
        "    return temp_df[name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5XqWkdpbcVS"
      },
      "outputs": [],
      "source": [
        "confirmed_diff_df = pd.DataFrame()\n",
        "countries = confirmed_ts_by_country.index\n",
        "\n",
        "\n",
        "for country in countries:\n",
        "    series_df = get_rate_of_change_with_ma(confirmed_ts_by_country.loc[country]).to_frame().T.fillna(0)\n",
        "\n",
        "    confirmed_diff_df = pd.concat([confirmed_diff_df, series_df], axis=0)\n",
        "    confirmed_diff_df.index = confirmed_diff_df.index[:-1].tolist() + [country]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhdrYghHeZtK"
      },
      "source": [
        "* The StandardScaler from sklearn is used to standardize the data before applying PCA. This scaling is done to ensure that each feature (in this case, each time point across all time series) has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "* Reason: The goal here is to prepare the data for PCA, which is sensitive to the variances of the original variables. Standardizing the data before PCA ensures that the principal components are not biased toward features with larger scales."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anomaly Detectiong"
      ],
      "metadata": {
        "id": "Mxjo6Hbcz-uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For countries that exhibit no significant rate of change or show a nearly straight line after 100 days, I will classify them as anomalies and exclude them from the clustering analysis.\n",
        "\n",
        "**Algorithm Explanation:**\n",
        "\n",
        "1. **Calculate Slope**: The algorithm calculates the slope between two points in the time series. Each point is defined by its timestamp and value. The slope is computed as the change in value divided by the change in time (in seconds).\n",
        "\n",
        "2. **Check for Anomalies**: For each country, the algorithm calculates slopes between points separated by a given period. It then scales these slopes to a range of [-1, 1] using MinMaxScaler. If the slope is below a certain threshold, it is considered significant. The slopes are checked for anomalies based on their scaled values.\n",
        "\n",
        "3. **Determine Longest Sequence of Anomalies**: The algorithm converts the list of slopes into binary values (1 if the slope is below the threshold, otherwise 0). It then finds the longest consecutive sequence of 1s (which represent anomalies).\n",
        "\n",
        "4. **Update Anomaly DataFrame**: If the longest sequence of anomalies for a country is greater than or equal to a specified length (e.g., 390), the country is flagged as an anomaly and included in the final results.\n"
      ],
      "metadata": {
        "id": "oUJ0jA03r9r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def calculate_slope(point1, point2):\n",
        "    \"\"\"\n",
        "    Calculate the slope between two points of a time series.\n",
        "\n",
        "    Parameters:\n",
        "    - point1: A tuple (x1, y1) where x1 is the time (as a timestamp) and y1 is the value at that time.\n",
        "    - point2: A tuple (x2, y2) where x2 is the time (as a timestamp) and y2 is the value at that time.\n",
        "\n",
        "    Returns:\n",
        "    - The slope between the two points.\n",
        "    \"\"\"\n",
        "    x1, y1 = point1\n",
        "    x2, y2 = point2\n",
        "\n",
        "    # Calculate the difference in time in seconds\n",
        "    time_diff = (x2 - x1).total_seconds()  # Converts the time difference to seconds\n",
        "\n",
        "    # Check for division by zero error\n",
        "    if time_diff == 0:\n",
        "        raise ValueError(\"x2 and x1 cannot be the same value (would cause division by zero).\")\n",
        "\n",
        "    # Calculate the slope as (change in y) / (change in time)\n",
        "    slope = (y2 - y1) / time_diff\n",
        "    return slope\n",
        "\n",
        "def check_anomaly(ts, period, threshold):\n",
        "    \"\"\"\n",
        "    Check for anomalies in a time series based on the slope between points.\n",
        "\n",
        "    Parameters:\n",
        "    - ts: The time series data.\n",
        "    - period: The period over which to calculate the slope.\n",
        "    - threshold: The threshold value for anomaly detection.\n",
        "\n",
        "    Returns:\n",
        "    - A list of scaled slopes between points.\n",
        "    \"\"\"\n",
        "    # Placeholder function to process the time series (assuming it processes the time series correctly)\n",
        "    ts = get_time_series(ts)\n",
        "\n",
        "    slope_ph = []\n",
        "\n",
        "    # Loop through the time series to calculate slopes between points\n",
        "    for i in range(0, len(ts) - period):\n",
        "        point_1 = (ts.index[i], ts.iloc[i])  # Start point\n",
        "        point_2 = (ts.index[i + period], ts.iloc[i + period])  # End point\n",
        "\n",
        "        # Calculate slope between the two points\n",
        "        slope = calculate_slope(point_1, point_2)\n",
        "\n",
        "        # Append slope to list if index is greater than 100\n",
        "        if i > 100:\n",
        "            slope_ph.append(slope)\n",
        "\n",
        "    # Scale slopes to range [-1, 1]\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaled_slope = scaler.fit_transform(np.array(slope_ph).reshape(-1, 1)).reshape(-1,)\n",
        "\n",
        "    return scaled_slope\n",
        "\n",
        "def longest_sequence_of_ones(lst):\n",
        "    \"\"\"\n",
        "    Find the length of the longest sequence of 1s in a list.\n",
        "\n",
        "    Parameters:\n",
        "    - lst: A list of binary values (0s and 1s).\n",
        "\n",
        "    Returns:\n",
        "    - The length of the longest sequence of 1s.\n",
        "    \"\"\"\n",
        "    max_count = 0  # Maximum length of sequence found\n",
        "    current_count = 0  # Current length of sequence\n",
        "\n",
        "    for num in lst:\n",
        "        if num == 1:\n",
        "            current_count += 1  # Increase the count of current sequence\n",
        "            max_count = max(max_count, current_count)  # Update max_count if current sequence is longer\n",
        "        else:\n",
        "            current_count = 0  # Reset count for sequence of 0s\n",
        "\n",
        "    return max_count\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "anomaly_df_confirm = pd.DataFrame(index=confirmed_diff_df.index, columns=['is_anomaly', 'long_seq'])\n",
        "anomaly_df_confirm['is_anomaly'] = 0\n",
        "\n",
        "# Loop through each country in the time series data\n",
        "countries = confirmed_diff_df.index\n",
        "for country in countries:\n",
        "    # Check for anomalies in the time series for each country\n",
        "    slope_list = check_anomaly(confirmed_diff_df.loc[country], 10, 0.0001)\n",
        "\n",
        "    # Convert slopes to binary based on a threshold\n",
        "    binary_list = [1 if x < 0.05 else 0 for x in slope_list]\n",
        "\n",
        "    # Find the length of the longest sequence of 1s\n",
        "    long_seq = longest_sequence_of_ones(binary_list)\n",
        "\n",
        "    # Update the DataFrame with the longest sequence length\n",
        "    anomaly_df_confirm.loc[country, 'long_seq'] = long_seq\n",
        "\n",
        "    # Mark as anomaly if the longest sequence is greater than or equal to 500\n",
        "    if long_seq >= 390:\n",
        "        anomaly_df_confirm.loc[country, 'is_anomaly'] = 1\n"
      ],
      "metadata": {
        "id": "zHp6Caz40CWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_df_confirm[anomaly_df_confirm.is_anomaly==1]"
      ],
      "metadata": {
        "id": "o09ygzT4BHT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies_countries_df = confirmed_diff_df.loc[anomaly_df_confirm[anomaly_df_confirm.is_anomaly==1].index]\n",
        "healthy_confirmed_df =  confirmed_diff_df[~confirmed_diff_df.index.isin(anomaly_df_confirm[anomaly_df_confirm.is_anomaly==1].index)]\n",
        "# Plot the data\n",
        "ax = anomalies_countries_df.T.plot(kind='line')\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Anomalies Countries')\n",
        "\n",
        "ax.legend().set_visible(False)\n",
        "\n",
        "# Adjust the layout to make room for the legend\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7kAqMm2dXSxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ-DXMzxHNqH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "standardized_data = scaler.fit_transform(healthy_confirmed_df.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrb9_YUlcAp7"
      },
      "outputs": [],
      "source": [
        "standardized_data.T.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxD1Cc0Jb__c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply PCA\n",
        "n_components = 20\n",
        "pca = PCA(n_components=n_components)\n",
        "pca_transformed_data = pca.fit_transform(standardized_data.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEKm55qZjN7S"
      },
      "source": [
        "Then check for the amount of variance explained by each component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2Si8EzSe8qK"
      },
      "outputs": [],
      "source": [
        "cumsum_explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, ax1 = plt.subplots(figsize=(20, 6))\n",
        "\n",
        "# Plot the explained variance ratio on the primary y-axis\n",
        "ax1.bar(range(1, pca_transformed_data.shape[1] + 1), pca.explained_variance_ratio_, alpha=0.7, color='b', label='Explained Variance Ratio')\n",
        "ax1.set_xlabel('Principal Component')\n",
        "ax1.set_ylabel('Explained Variance Ratio', color='b')\n",
        "ax1.tick_params(axis='y', labelcolor='b')\n",
        "ax1.set_title('PCA Explained Variance Ratio')\n",
        "ax1.set_xticks(range(1, pca_transformed_data.shape[1] + 1))\n",
        "ax1.grid(axis='y')\n",
        "ax1.tick_params(axis='x', labelsize=10)\n",
        "\n",
        "# Create a secondary y-axis and plot the cumulative sum on it\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(1, pca_transformed_data.shape[1] + 1), cumsum_explained_variance_ratio, marker='o', color='r', label='Cumulative Sum')\n",
        "ax2.set_ylabel('Cumulative Sum', color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "# Adding legends for both axes\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boAdFbb6X9bA"
      },
      "outputs": [],
      "source": [
        "n_components = 6\n",
        "pca = PCA(n_components=n_components)\n",
        "pca_transformed_data = pca.fit_transform(standardized_data.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uHulZ1ziElu"
      },
      "outputs": [],
      "source": [
        "pca_transformed_data.reshape(pca_transformed_data.shape[0],pca_transformed_data.shape[1],1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBH8e4zXE2SW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Sum_of_squared_distances = []\n",
        "silhouette_scores = []\n",
        "\n",
        "K = range(3,20)\n",
        "for k in K:\n",
        "\n",
        "    km = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\",init=\"k-means++\")\n",
        "    km.fit(pca_transformed_data.reshape(pca_transformed_data.shape[0],pca_transformed_data.shape[1],1))\n",
        "    labels = km.predict(pca_transformed_data.reshape(pca_transformed_data.shape[0],pca_transformed_data.shape[1],1))\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "    score = silhouette_score(pca_transformed_data.reshape(pca_transformed_data.shape[0],pca_transformed_data.shape[1],1), labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f'N Cluster:{k}, inertia: {km.inertia_}, silhouette_scores: {score}')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMStBWaV9aUJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "Sum_of_squared_distances = []\n",
        "silhouette_scores = []\n",
        "\n",
        "K = range(3,20)\n",
        "for k in K:\n",
        "\n",
        "    km = KMeans(n_clusters=k,init=\"k-means++\")\n",
        "    km.fit(pca_transformed_data)\n",
        "    labels = km.predict(pca_transformed_data)\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "    score = silhouette_score(pca_transformed_data, labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f'N Cluster:{k}, inertia: {km.inertia_}, silhouette_scores: {score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHWla1jY-feG"
      },
      "source": [
        "Using elbow method to get number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uPR42A4WbZh"
      },
      "outputs": [],
      "source": [
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX5sIymvfBae"
      },
      "outputs": [],
      "source": [
        "pca_transformed_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnRlilrSZNoC"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 17\n",
        "km = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\",init=\"k-means++\")\n",
        "km.fit(pca_transformed_data.reshape(pca_transformed_data.shape[0],pca_transformed_data.shape[1],1))\n",
        "labels = km.predict(pca_transformed_data.reshape(pca_transformed_data.shape[0],pca_transformed_data.shape[1],1))\n",
        "mat = pd.DataFrame(pca_transformed_data)\n",
        "mat['cluster'] = pd.Series(labels)\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n",
        "\n",
        "LABEL_COLOR_MAP = {\n",
        "    0:'r',1: 'tan', 2: 'b', 3: 'k', 4: 'c', 5: 'g', 6: 'deeppink', 7: 'skyblue', 8: 'darkcyan', 9: 'orange',\n",
        "    10: 'yellow', 11: 'tomato', 12: 'seagreen', 13: 'purple', 14: 'olive', 15: 'magenta', 16: 'navy',\n",
        "    17: 'gold', 18: 'coral', 19: 'lime', 20: 'crimson', 21: 'teal', 22: 'orchid', 23: 'salmon',\n",
        "    24: 'sienna', 25: 'peru', 26: 'lavender', 27: 'turquoise', 28: 'maroon', 29: 'chocolate', 30: 'indigo'\n",
        "}\n",
        "label_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n",
        "\n",
        "fig = plt.figure(figsize = (12,10))\n",
        "plt.subplots_adjust(left=0.1, bottom=0.05, right=0.85, top=0.95, wspace=0.2, hspace=0.4)\n",
        "increment = 0\n",
        "for ix in range(6):\n",
        "    for iy in range(ix+1, 6):\n",
        "        increment += 1\n",
        "        ax = fig.add_subplot(4,3,increment)\n",
        "        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.5)\n",
        "        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n",
        "        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n",
        "        ax.yaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.xaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "\n",
        "        if increment == 12: break\n",
        "    if increment == 12: break\n",
        "\n",
        "#_______________________________________________\n",
        "\n",
        "comp_handler = []\n",
        "for i in range(n_clusters):\n",
        "    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n",
        "\n",
        "plt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.9),\n",
        "           title='Cluster', facecolor = 'lightgrey',\n",
        "           shadow = True, frameon = True, framealpha = 1,\n",
        "           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGzJN29ypfN2"
      },
      "source": [
        "PCA didnt help that much to know number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_sxP42XWiv6"
      },
      "outputs": [],
      "source": [
        "#first scale the data\n",
        "\n",
        "#scaler_ts = TimeSeriesScalerMeanVariance()\n",
        "#scaled_data = scaler_ts.fit_transform(healthy_confirmed_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhJJAPSeqjRX"
      },
      "source": [
        "Using MinMaxScaler instead of TimeSeriesScalerMeanVariance becuase it give better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-8SpHJvHo7X"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(healthy_confirmed_df.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caYiC6SS3km"
      },
      "outputs": [],
      "source": [
        "#squeezed_array = np.squeeze(scaled_data, axis=-1)\n",
        "\n",
        "# Convert to a DataFrame\n",
        "scaled_data_df = pd.DataFrame(scaled_data.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "texlw5_maEW4"
      },
      "source": [
        "Using Dendrogram to know number of clusters needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nyk2CsDRYnY"
      },
      "outputs": [],
      "source": [
        "from tslearn.metrics import cdist_dtw\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "data_array = scaled_data.T.copy()\n",
        "\n",
        "distance_matrix = cdist_dtw(data_array, data_array)\n",
        "\n",
        "linked = linkage(squareform(distance_matrix), method='ward')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz6QsetGRmdG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(18, 7))\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrFD36tcaWGk"
      },
      "source": [
        "applying clusters using eclidean i used DTW was so bad and eclidian gave better results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data.shape"
      ],
      "metadata": {
        "id": "rY0xy8z2cfJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk-fLivatQPg"
      },
      "outputs": [],
      "source": [
        "# Clustring\n",
        "kmeans_ts_final  = TimeSeriesKMeans(n_clusters=8, metric=\"euclidean\",init=\"k-means++\",random_state=10,n_init=30)\n",
        "final_clusters  = kmeans_ts_final.fit_predict(scaled_data.T.reshape(scaled_data.shape[1],scaled_data.shape[0],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efOI0rHeFv-E"
      },
      "outputs": [],
      "source": [
        "scaled_data_df.index=healthy_confirmed_df.index\n",
        "scaled_data_df['cluster'] = final_clusters\n",
        "clusters = scaled_data_df['cluster'].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py0ktEEQpoHx"
      },
      "source": [
        "## Overview of the Algorithm\n",
        "\n",
        "1. **Initialization:**\n",
        "   - Create a DataFrame to track country names associated with different cluster configurations.\n",
        "\n",
        "2. **Iterate Over Possible Cluster Counts:**\n",
        "   - For each potential number of clusters (from 2 up to a specified maximum depth), perform clustering on the data.\n",
        "\n",
        "3. **Perform Clustering:**\n",
        "   - For each cluster count:\n",
        "     - Apply the KMeans clustering algorithm to the data.\n",
        "     - Assign cluster labels to each data point based on the clustering result.\n",
        "     - Associate each data point with its respective cluster and country name.\n",
        "\n",
        "4. **Aggregate Cluster Data:**\n",
        "   - Group the data by cluster labels and compile a list of countries for each cluster.\n",
        "   - Update the DataFrame to store concatenated strings of countries for each cluster configuration.\n",
        "\n",
        "5. **Concatenate and Evaluate:**\n",
        "   - Create a new column that concatenates values from different cluster configurations for each data point, separated by underscores.\n",
        "   - Determine the second most frequent country (or cluster) for each data point based on the concatenated strings.\n",
        "\n",
        "6. **Assign Core Time Series:**\n",
        "   - For each cluster:\n",
        "     - Identify the most common core time series (the second most frequent value from the concatenation).\n",
        "     - Assign this core time series to the data points in that cluster.\n",
        "\n",
        "7. **Close Vote:**\n",
        "   - Determine which core time series is closest to the values assigned to each data point by checking the frequency of occurrence.\n",
        "   - Update the DataFrame to include this closest vote for each data point.\n",
        "\n",
        "8. **Return Improved Data:**\n",
        "   - Return the updated DataFrame with improved cluster assignments and core time series information, removing intermediate columns used for calculations.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The algorithm enhances clustering results by exploring various cluster counts, aggregating and analyzing data to identify the most representative clusters, and refining cluster assignments based on a voting mechanism. This process aims to identify more meaningful clusters and improve overall clustering accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF0vmyu0pnoj"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def improving_clusters(df,max_depth=100):\n",
        "  vote_df = pd.DataFrame(data=df.index,columns=['countries'])\n",
        "  for i in range(2,max_depth+1):\n",
        "    print(i)\n",
        "    temp_data = df.iloc[:,:1143].copy()\n",
        "    n_cluster=i\n",
        "    km = KMeans(n_clusters=n_cluster,init=\"k-means++\",random_state=10,n_init=30)\n",
        "    km.fit(temp_data.values)\n",
        "    labels = km.predict(temp_data.values)\n",
        "    temp_data['cluster'] = labels\n",
        "    temp_data['countries'] = temp_data.index\n",
        "    df_countries = temp_data[['cluster','countries']].groupby('cluster').agg(list).copy()\n",
        "    for label in labels:\n",
        "      cluster_index = temp_data.cluster==label\n",
        "      vote_df.loc[cluster_index.values,f'{n_cluster}'] = '_'.join(df_countries.loc[label].countries)\n",
        "\n",
        "\n",
        "  vote_df['Concatenated'] = vote_df.drop('countries',axis=1).apply(lambda row: '_'.join(row), axis=1)\n",
        "  def voting(x):\n",
        "    my_list = x.split('_')\n",
        "    counter = Counter(my_list)\n",
        "    most_common = counter.most_common()\n",
        "    return most_common[1]\n",
        "  vote_df['voting'] = vote_df['Concatenated'].apply(voting)\n",
        "  vote_df['voting_score'] = vote_df['voting'].apply(lambda x: x[1])\n",
        "  vote_df['voting'] = vote_df['voting'].apply(lambda x: x[0])\n",
        "  vote_df.index=vote_df['countries']\n",
        "  vote_df.drop('countries',axis=1,inplace=True)\n",
        "\n",
        "  clusters = df.cluster.unique()\n",
        "  for n in clusters:\n",
        "    cluster_index = df.cluster==n\n",
        "    #vote_df.loc[df[df.cluster==0].index][['voting','voting_score']].groupby('voting').sum().sort_values('voting_score',ascending=False).index[0]\n",
        "    df.loc[cluster_index.values,'Core Time Series'] = vote_df.loc[df[df.cluster==n].index][['voting','voting_score']].groupby('voting').sum().sort_values('voting_score',ascending=False).index[0]\n",
        "\n",
        "  clusters_votes = df['Core Time Series'].unique()\n",
        "  def close_vote(x):\n",
        "    my_list = x.split('_')\n",
        "    counter = Counter(my_list)\n",
        "    most_common = counter.most_common()\n",
        "    return most_common\n",
        "  vote_df['Close Vote'] = None\n",
        "  df['Close Vote'] = None\n",
        "  for r,row in vote_df.iterrows():\n",
        "    temp_list = row['Concatenated']\n",
        "    temp_vote = close_vote(temp_list)\n",
        "    for i in temp_vote:\n",
        "      if i[0] in clusters_votes:\n",
        "        vote_df.at[r,'Close Vote'] = i[0]\n",
        "        df.at[r,'Close Vote'] = df[df['Core Time Series'] == i[0]]['cluster'].unique()[0]\n",
        "        break\n",
        "  return df.drop(['cluster','Core Time Series'],axis=1).rename(columns={'Close Vote':'cluster'})\n",
        "improved_clusters = improving_clusters(scaled_data_df.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57M0idMHbA5a"
      },
      "source": [
        "#Visualize clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HyRrnkp7esu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ensure your dataframes are correctly sorted and ready for plotting\n",
        "scaled_data_df = scaled_data_df.sort_values('cluster')\n",
        "improved_clusters = improved_clusters.sort_values('cluster')\n",
        "\n",
        "clusters = scaled_data_df['cluster'].unique()\n",
        "num_clusters = len(clusters)\n",
        "\n",
        "# Create subplots for each cluster pair (original and improved) on the same row\n",
        "fig, axes = plt.subplots(num_clusters, 2, figsize=(20, 6 * num_clusters), sharex=True)\n",
        "\n",
        "# Iterate over clusters and plot each one\n",
        "for i, cluster in enumerate(clusters):\n",
        "    # Plot for original scaled_data_df clusters\n",
        "    ax = axes[i, 0]  # Access the subplot in the ith row, first column\n",
        "    cluster_data = scaled_data_df[scaled_data_df['cluster'] == cluster]\n",
        "\n",
        "    # Ensure centroid data is numeric\n",
        "    centroid = kmeans_ts_final.cluster_centers_[cluster].ravel()\n",
        "    ax.plot(centroid, \"r-\", linewidth=2, label='Centroid')\n",
        "\n",
        "    for index, row in cluster_data.iterrows():\n",
        "        # Extract only numeric data for plotting\n",
        "        time_series = row.drop(['cluster', 'Core Time Series'], errors='ignore')  # Drop the non-numeric column(s)\n",
        "        ax.plot(time_series, label=f'Row {index}', alpha=.3, linewidth=2)\n",
        "\n",
        "    ax.set_title(f'Original Cluster {cluster}')\n",
        "    ax.set_ylabel('Value')\n",
        "\n",
        "    # Plot for improved_clusters\n",
        "    ax = axes[i, 1]  # Access the subplot in the ith row, second column\n",
        "    improved_cluster_data = improved_clusters[improved_clusters['cluster'] == cluster]\n",
        "\n",
        "    # Ensure centroid data is numeric\n",
        "    ax.plot(centroid, \"r-\", linewidth=2, label='Centroid')\n",
        "    median_row = improved_cluster_data.drop('cluster',axis=1).median()\n",
        "    ax.plot(median_row, \"b-\", linewidth=2, label='meidan')\n",
        "    for index, row in improved_cluster_data.iterrows():\n",
        "        # Extract only numeric data for plotting\n",
        "        time_series = row.drop(['cluster'], errors='ignore')  # Drop the non-numeric column(s)\n",
        "        ax.plot(time_series, label=f'Row {index}', alpha=.3, linewidth=2)\n",
        "\n",
        "    ax.set_title(f'Improved Cluster {cluster}')\n",
        "    ax.set_ylabel('Value')\n",
        "\n",
        "# Add x-axis label to the bottom subplot\n",
        "for ax in axes[-1, :]:\n",
        "    ax.set_xlabel('Time Points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Improved Cluster method discovered that no need for cluster 6 and can fit in other clusters"
      ],
      "metadata": {
        "id": "zbUqoubD-v2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "def calculate_wcss(df):\n",
        "    \"\"\"\n",
        "    Calculate the Within-Cluster Sum of Squares (WCSS) for the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with time series data where the last column is cluster labels.\n",
        "\n",
        "    Returns:\n",
        "    - WCSS value.\n",
        "    \"\"\"\n",
        "    X = df.iloc[:, :-1].values  # Time series data\n",
        "    labels = df.iloc[:, -1].values  # Cluster labels\n",
        "\n",
        "    wcss = 0\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    for label in unique_labels:\n",
        "        cluster_data = X[labels == label]\n",
        "        cluster_center = cluster_data.mean(axis=0)\n",
        "        wcss += np.sum((cluster_data - cluster_center) ** 2)\n",
        "\n",
        "    return wcss\n",
        "\n",
        "def evaluate_clustering(df):\n",
        "    \"\"\"\n",
        "    Evaluate clustering quality metrics and create a silhouette plot for the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with time series data where the last column is cluster labels.\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary with clustering metrics and silhouette plot.\n",
        "    \"\"\"\n",
        "    X = df.iloc[:, :-1].values  # Time series data\n",
        "    labels = df.iloc[:, -1].values  # Cluster labels\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # Calculate WCSS\n",
        "    metrics['WCSS'] = calculate_wcss(df)\n",
        "\n",
        "    # Calculate Silhouette Score\n",
        "    metrics['Silhouette Score'] = silhouette_score(X, labels)\n",
        "\n",
        "    # Calculate Davies-Bouldin Index\n",
        "    metrics['Davies-Bouldin Index'] = davies_bouldin_score(X, labels)\n",
        "\n",
        "    # Calculate Calinski-Harabasz Index\n",
        "    metrics['Calinski-Harabasz Index'] = calinski_harabasz_score(X, labels)\n",
        "\n",
        "    # Create Silhouette Plot\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    silhouette_vals = silhouette_score(X, labels)\n",
        "\n",
        "    # Plot silhouette scores\n",
        "    plt.subplot(1, 2, 1)\n",
        "    silhouette_avg = np.mean(silhouette_vals)\n",
        "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    plt.hist(silhouette_vals, bins=20, edgecolor='k', alpha=0.7)\n",
        "    plt.title('Silhouette Plot')\n",
        "    plt.xlabel('Silhouette Score')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def compare_clustering(df1, df2):\n",
        "    \"\"\"\n",
        "    Compare clustering quality between two DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "    - df1: First DataFrame with time series data where the last column is cluster labels (original clustering).\n",
        "    - df2: Second DataFrame with time series data where the last column is cluster labels (improved clustering).\n",
        "\n",
        "    Returns:\n",
        "    - Comparison of clustering metrics.\n",
        "    \"\"\"\n",
        "    print(\"Evaluating Clustering for Original Clustering:\")\n",
        "    metrics1 = evaluate_clustering(df1)\n",
        "    for metric, value in metrics1.items():\n",
        "        print(f\"{metric} for Original Clustering: {value}\")\n",
        "\n",
        "    print(\"\\nEvaluating Clustering for Improved Clustering:\")\n",
        "    metrics2 = evaluate_clustering(df2)\n",
        "    for metric, value in metrics2.items():\n",
        "        print(f\"{metric} for Improved Clustering: {value}\")\n",
        "\n",
        "    # Plot silhouette scores for both dataframes\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Silhouette Comparison')\n",
        "    plt.bar(['Original Clustering', 'Improved Clustering'],\n",
        "            [metrics1['Silhouette Score'], metrics2['Silhouette Score']],\n",
        "            color=['blue', 'green'])\n",
        "    plt.ylabel('Silhouette Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_clustering(scaled_data_df, improved_clusters)\n"
      ],
      "metadata": {
        "id": "q6kLEAdy8Uq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqSagvzvni_j"
      },
      "outputs": [],
      "source": [
        "improved_clusters['cluster'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Cluster Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGG5L8TZRTlh"
      },
      "source": [
        "Visualize clusters on the world map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKxwF_bcQ8wE"
      },
      "outputs": [],
      "source": [
        "improved_clusters['categ'] = improved_clusters['cluster'].astype(str)\n",
        "improved_clusters.sort_values('cluster',inplace=True)\n",
        "fig = px.choropleth(\n",
        "    improved_clusters.reset_index(),\n",
        "    locations='index',\n",
        "    locationmode='country names',\n",
        "    color='categ',\n",
        "    hover_name='index',\n",
        "    title='Choropleth Map of Clusters by Country',\n",
        "    color_discrete_sequence=px.colors.qualitative.Set3  # Use a discrete color set\n",
        ")\n",
        "\n",
        "fig.update_geos(projection_type=\"natural earth\")\n",
        "fig.update_layout(\n",
        "    geo=dict(showframe=False, showcoastlines=False),\n",
        "    width=1200,\n",
        "    height=800\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjacent countries tend to have same cluster which emphasize clustring quality"
      ],
      "metadata": {
        "id": "ifo1NCzyD-Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Future Improvements\n",
        "\n",
        "To enhance the accuracy and reliability of the clustering results, future improvements could include:\n",
        "\n",
        "- **Incorporate Dynamic Time Warping (DTW):** Implementing DTW for distance measurement could improve the capture of temporal patterns in the data. Unlike Euclidean distance, DTW is better suited for aligning time series data that may vary in speed or timing, making it a more accurate method for identifying similar patterns in COVID-19 case data.\n",
        "\n",
        "- **Implement Cross-Validation Techniques:** To ensure the robustness of the clustering results, applying cross-validation techniques can help validate the model by testing its performance on different subsets of data. This would help in verifying that the clustering model generalizes well to unseen data and isn't overfitted to the specific dataset used.\n",
        "\n",
        "By implementing these enhancements, the project could yield even more reliable insights, making it better suited for real-world applications and further research.\n"
      ],
      "metadata": {
        "id": "E5tM1lR3_blM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}